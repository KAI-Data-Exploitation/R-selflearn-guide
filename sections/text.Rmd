
# Working with text

Text data can be explored in a number of different ways: identifying the most common words or phrases; sentiment analysis; using machine learning to split the documents into different topics (a form of machine learning); or, more generally, using text as features in a predictive model.

There are a number of packages in R for working with text. `tm` is the most established and will be the focus of the discussion here but many new packages have been released, with `text2vec` in particular looking very promising due to its simple interface and its inclusion of advanced techniques.

Before text can be used it needs to be preprocessed, which usually means cleaning the text and converting words into their root forms by either stemming or lemmatisation.

Resources:

- [tm package documentation](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)
- [An example](https://www.r-bloggers.com/text-mining-the-complete-works-of-william-shakespeare/) of text mining
- [A quick introduction](https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/) to text mining in R
- [Text Mining with R](https://www.tidytextmining.com/) focuses on using the `tidytext` package to analyse text. This is particularly good for sentiment analysis
- More information about the `text2vec` package can be found [here](http://text2vec.org/).


## Preprocessing

A package you can use for stemming and cleaning data is:  **tm - Package for Text Mining**.  


### Reading data into tm

Data can be made into a *corpus* (the main data structure `tm` uses) using either the `VectorSource()` function that reads a list of strings or `DirSource` which reads documents from a directory. For example, if you have a csv file called *example.csv* with a column of text called *text* you could run:
```{r, eval = FALSE, message=FALSE}
library(readr)
library(tm)

df <- read_csv('example.csv')
tm_corpus <- VCorpus(VectorSource(df$text))
```


### Cleaning text

The following functions can be used in this package to clean your data:  

- `tm_map(corpus, content_transformer(to_lower))` will remove all capital letters and replace them with lowercase.  
- `tm_map(corpus, removePunctuation)` will remove all punctuation.  
- `tm_map(corpus, removeNumbers)` will remove all numbers.  
- `tm_map(corpus, removeWords, all_stop)` takes a list of words that you wish to exclude from the data and removes them. R already has built-in lists of common words such as 'i', 'me', 'you', and 'he'. In this case we have used a personalised list called 'all_stop'.  
- `tm_map(corpus, stripWhitespace)` will remove any spacing that has occurred when removing numbers, punctuation, *etc*.  
  
Below is an example of a function taking a corpus of documents, cleaning and stemming it and assigning it as `cleaned_corpus`.
```{r, results=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(dplyr)

stopwds <- stopwords('en')
all_stop <- c("concern", "concerned", "concerns", "may", "also",
              "will", "see", "around","yet","though", stopwds)

clean_corpus <- function(corpus){
  corpus %>%
  tm_map(content_transformer(tolower)) %>%    
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, all_stop) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument) -> clean_corpus
  return(clean_corpus)
}
```

This can then be applied on a corpus.
```{r eval=FALSE}
cleaned_corpus <- clean_corpus(tm_corpus)
```

You can also write your own functions to be applied to the data using the `tm_map()` function, for example you might apply regular expressions created with the `stringr` package. Often these functions will have to be wrapped within the `content_transformer()` function (see the `tolower()` example above).

### Stemming and lemmatisation

Stemming is the process of reducing words to their stem, base or root form. This is done to group together similar words so that their frequencies reflect the true use of the word, another more sophisticated version of this is lemmatisation.

Stemming just removes the end of the word to approximate the root and can be done in `tm` using the following command:
```{r, eval=FALSE}
tm_corpus %>% tm_map(stemDocument)
```

Lemmatisation finds the root word itself but is much slower. For example,
```{r, eval=FALSE}
library(textstem)
corpus %>% tm_map(lemmatize_strings)
```


### Spelling
Misspelt text can lead to underestimates of the prevalence of terms. The `hunspell` package can check spelling and suggest correct alternatives.

```{r}

library(hunspell)

bad <- hunspell("spell checkers are not neccessairy for langauge ninja's")
print(bad[[1]])
hunspell_suggest(bad[[1]])

```

## Visualisations

Common visualisations used to explore text data include word frequency charts and word clouds. These are demonstrated below, where the *crude* dataset (a corpus of documents about crude oil provided by the `tm` package) has been used .

### Word frequency charts

Once your data has been cleaned you can start creating visualisations. One of these could be a word frequency chart. To do this we will use the `ggplot2` package.  
After cleaning your data you'll have to create a document-term matrix using `tm`. This is a matrix whose elements indicate the number of times a given word (or term) has appeared within a given document. The columns denote words and the rows denote documents. The code below generate a document-term matrix on the *crude* dataset.  
```{r results=FALSE, message=FALSE, warning=FALSE, error=FALSE}
library(tm)
library(dplyr)
library(tibble)

data(crude)
crude <- clean_corpus(crude)

dtm <- DocumentTermMatrix(crude)
```
You will then need to select the word frequencies you want. The code below will generate word counts. 

```{r results=FALSE}
word_counts <- 
  data.frame(freq = dtm %>% as.matrix() %>% colSums()) %>% 
  rownames_to_column("word")
```
You are now ready to create your word frequency chart. The code below creates a word-frequency chart displaying the ten most frequently occuring words.  

```{r warning=FALSE, message=FALSE, fig.align='center', fig.width=4, fig.height=3.5}
library(ggplot2)

wf <- word_counts %>% top_n(10, freq) 

ggplot(wf, aes(x = reorder(word, -freq), y = freq, fill="")) +
  geom_bar(stat = "identity", colour="black") + 
  scale_fill_manual(values=c("#3399FF")) +
  theme(axis.text.x=element_text(angle=90, hjust=1),
        axis.title.x = element_blank(),
        legend.position = "none")
```

### Word clouds

Another way you can visualise your data is by using a word cloud. Word clouds do have their limitations but are good for picking out words at a quick glance. A package you can use to create these is `ggwordcloud`. This package provides a couple of shortcut functions to quickly produce wordclouds (`ggwordcloud()` and `ggwordcloud2()`) and also extends extends `ggplot2` by providing a number of wordcloud geoms. Using these geoms requires a bit more code but gives you full control over your wordcloud; for example, it provides control over whether the size of the text scales with the word frequency or the square of the frequency and over the shape of the overall wordcloud.

Below we create a word cloud of all words appearing more than two times in the corpus. Note that in this example the word frequencies need to be sorted so that the most commonly occuring words appear at the start of the dataframe.
```{r warning=FALSE, message=FALSE, fig.align='center'}
library(ggwordcloud)

words_filtered <- word_counts %>% filter(freq > 2) %>% arrange(desc(freq))
ggwordcloud2(words_filtered, shuffle = F, size = 2.5, ellipticity = 0.9)
```

More information about plotting with `ggplot2` can be found in the [Creating charts](#charts) section and more information about `ggwordcloud` [can be found here](https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html).


## Machine learning

Machine learning can be used to try and detect patterns across text documents (*topic modelling*) and text can also be used as features in predictive models.

### Topic modelling

Latent Dirichlet Allocation (LDA) is an unsupervised learning technique for identifying topics within a corpus. This is implemented in the `topicmodels` package. For example, the code below looks to identify five topics within the document-term matrix `dtm`.
```{r eval=FALSE}
library(topicmodels)

lda <- LDA(dtm, k = 5)
```

Once topics have been identified they can be explored interactively using the `LDAvis` package. [This post on stackoverflow](https://stackoverflow.com/a/45350283) demonstrates how to convert the output of `topicmodels` into the format required by `LDAvis`.

Resources:

- [Tidy Text Mining with R - Topic Modelling](https://www.tidytextmining.com/topicmodeling.html)  
- [Introduction to Topic Modelling](https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/) 

### Text features

For other types of machine learning you will want to convert the document-term matrix into a dataframe of features. However, the document-term matrix may contain features that do not appear very often and that you want to remove. This can be done with the `removeSparseTerms()` function before converting to a dataframe.

```{r eval=FALSE}
dtm = removeSparseTerms(dtm, 0.99)
dtm <- as.data.frame(as.matrix(dtm))
```


## N-grams

An n-gram is a list of n sequential words taken from a document. For example, the phrase 

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "the quick brown fox jumps over the lazy dog"

contains the bigrams

  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "the quick", "quick brown", "brown fox", *etc*.

These are often useful to visualise in order to understand phrases in the data or as additional features for machine learning models.

The package `tidytext` can be used to produce a dataframe of bigrams that is useful for data exploration and data visualisation:

```{r eval=FALSE}
library(tidytext)

bigrams <- tm_corpus %>% 
  tm_map(PlainTextDocument) %>%
  tidy() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

The [tm FAQ](http://tm.r-forge.r-project.org/faq.html) shows how to include bigrams within a document-term matrix.